{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9596abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv2D , Dropout, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632b1fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 574\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"E:/Jupyter/masterthesis/models/two_classes/cnn_2class_augmented.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7cd0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset paths\n",
    "TRAINING_DIR = 'E:/Jupyter/masterthesis/data/model_data_logScaleMelSpectrogram/two_class/augmented/train'\n",
    "VALIDATION_DIR = 'E:/Jupyter/masterthesis/data/model_data_logScaleMelSpectrogram/two_class/augmented/val'\n",
    "TEST_DIR = 'E:/Jupyter/masterthesis/data/model_data_logScaleMelSpectrogram/two_class/normal/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(train_path, img_size, batch_size):\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(\n",
    "            rescale = 1./255)\n",
    "\n",
    "    train_dataset = train_datagen.flow_from_directory(\n",
    "            train_path,\n",
    "            target_size = (img_size, img_size),\n",
    "            batch_size = batch_size,\n",
    "            seed = 42,\n",
    "            class_mode = 'categorical',\n",
    "            shuffle = True)\n",
    "\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e303418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_val_data(val_path, img_size, batch_size):\n",
    "\n",
    "    val_datagen = ImageDataGenerator(\n",
    "            rescale = 1./255)\n",
    "\n",
    "    val_dataset = val_datagen.flow_from_directory(\n",
    "            val_path,\n",
    "            target_size = (img_size, img_size),\n",
    "            batch_size = batch_size,\n",
    "            seed = 42,\n",
    "            class_mode = 'categorical',\n",
    "            shuffle = True )\n",
    " \n",
    "    return val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc39503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data(test_path, img_size):\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(\n",
    "            rescale = 1./255)\n",
    "\n",
    "    test_dataset = test_datagen.flow_from_directory(\n",
    "            test_path,\n",
    "            target_size = (img_size, img_size),\n",
    "            batch_size = 1,\n",
    "            seed = 42,\n",
    "            class_mode = 'categorical',\n",
    "            shuffle = False )\n",
    "    \n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c7f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    \n",
    "    weight_initializer = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 0.01, seed = None)\n",
    "    bias_initializer = tf.keras.initializers.Zeros()\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # input layer\n",
    "    model.add(Conv2D(32, 3, padding = 'same', activation = 'relu', input_shape = input_shape, kernel_initializer = weight_initializer, bias_initializer = bias_initializer))\n",
    "    model.add(MaxPooling2D(pool_size = (3, 3), strides = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    \n",
    "    model.add(Conv2D(64, 3, padding = 'same', activation = 'relu', kernel_initializer = weight_initializer, bias_initializer = bias_initializer))\n",
    "    model.add(MaxPooling2D(pool_size = (3, 3), strides = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    \n",
    "    model.add(Conv2D(128, 3, padding = 'same', activation = 'relu', kernel_initializer = weight_initializer, bias_initializer = bias_initializer))\n",
    "    model.add(MaxPooling2D(pool_size = (3, 3), strides=(2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    \n",
    "    model.add(Conv2D(128, 3, padding = 'same', activation ='relu', kernel_initializer = weight_initializer, bias_initializer = bias_initializer))\n",
    "    model.add(MaxPooling2D(pool_size = (3, 3), strides = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation = 'relu', kernel_initializer = weight_initializer, bias_initializer = bias_initializer))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5908e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    \n",
    "    acc = history.history['categorical_accuracy']\n",
    "    val_acc = history.history['val_categorical_accuracy']\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    plt.figure(figsize = (8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(acc, label = 'Training Accuracy')\n",
    "    plt.plot(val_acc, label = 'Validation Accuracy')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(loss, label = 'Training Loss')\n",
    "    plt.plot(val_loss, label = 'Validation Loss')\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42437185",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # prepare data for training, validation and testing\n",
    "    train_set = prepare_train_data(TRAINING_DIR, IMG_SIZE, BATCH_SIZE)\n",
    "    val_set = prepare_val_data(VALIDATION_DIR, IMG_SIZE, BATCH_SIZE)\n",
    "    test_set = prepare_test_data(TEST_DIR, IMG_SIZE)\n",
    "    \n",
    "    # define input shape to the model\n",
    "    input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "    \n",
    "    # build model\n",
    "    model = build_model(input_shape)\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.CategoricalAccuracy()], \n",
    "                  sample_weight_mode=[None])\n",
    "    \n",
    "    # model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # The training stops if there is no improvement in the validation accuracy after 10 epoch\n",
    "    es = EarlyStopping(monitor='val_categorical_accuracy', patience = 10)\n",
    "    \n",
    "    # if there is a better validation accuracy than previous better accuracy, then we save it in the model\n",
    "    chkp = ModelCheckpoint(filepath=MODEL_PATH, verbose=1, save_best_only=True)\n",
    "    \n",
    "    # cnn_normal_log.csv has the accuracy and loss history\n",
    "    log = CSVLogger('E:/Jupyter/masterthesis/logs/two_classes/augmented/cnn_2class_augmented_log.csv', append=True, separator=' ')\n",
    "    \n",
    "    # train model \n",
    "    history = model.fit(train_set,\n",
    "        validation_data = val_set,\n",
    "        epochs = 50,\n",
    "        verbose = 1,\n",
    "        shuffle = True,\n",
    "        callbacks = [chkp,es,log])\n",
    "    \n",
    " \n",
    "    \n",
    "    # initializing time stamp\n",
    "    startTime = time.time()\n",
    "\n",
    "    # Loading the saved Model\n",
    "    model = tf.keras.models.load_model(MODEL_PATH)\n",
    "    model.summary()\n",
    "    \n",
    "    # Evaluation on validation dataset\n",
    "    score_val = model.evaluate(val_set)\n",
    "    print(model.metrics_names)\n",
    "    print(model.metrics_names[0], score_val[0])\n",
    "    print(model.metrics_names[1], score_val[1])\n",
    "\n",
    "\n",
    "    \n",
    "    # Evaluation on test dataset\n",
    "    score_test = model.evaluate(test_set)\n",
    "    print(model.metrics_names)\n",
    "    print(model.metrics_names[0], score_val[0])\n",
    "    print(model.metrics_names[1], score_val[1])\n",
    "    \n",
    "    # Prediction on test dataset\n",
    "    test_set.reset()\n",
    "    \n",
    "    pred = model.predict(test_set, batch_size = 32, verbose=2)\n",
    "    p_i = np.argmax(pred, axis=1)\n",
    "    \n",
    "    class_labels = (train_set.class_indices)\n",
    "    class_labels = dict((v,k) for k,v in class_labels.items())\n",
    "    \n",
    "    predictions = [class_labels[k] for k in p_i]\n",
    "    #predictions = predictions[:200]\n",
    "    \n",
    "    files = test_set.filenames\n",
    "    print(len(files), len(predictions))\n",
    "    \n",
    "    # Saving the predictions in a csv file\n",
    "    results = pd.DataFrame({\"Filename\":files,\n",
    "                      \"Predictions\":predictions})\n",
    "    results.to_csv(\"E:/Jupyter/masterthesis/predictions/two_class/augmented/cnn_2class_augmented_result.csv\",index=False)\n",
    "\n",
    "    executionTime = (time.time() - startTime)\n",
    "    print('\\n\\n\\n\\nExecution time in seconds: ' + str(executionTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "pred_file = \"E:/Jupyter/masterthesis/predictions/two_class/augmented/CNN_2class_augmented_result.csv\"\n",
    "df = pd.read_csv(pred_file)\n",
    "df['Target'] = df['Filename'].str.extract('(down_clicks|up_clicks)', expand = False)\n",
    "df[\"Predicted Labels\"] = df['Predictions'].str.extract('(down_clicks|up_clicks)', expand = False)\n",
    "\n",
    "\n",
    "df_new = df[['Target', 'Predicted Labels']]\n",
    "\n",
    "mapping = {'down_clicks': 0, 'up_clicks': 1}\n",
    "df1 = df_new.replace({'Target': mapping, 'Predicted Labels': mapping})\n",
    "\n",
    "y_test = df_new[\"Target\"]\n",
    "y_pred = df_new[\"Predicted Labels\"]\n",
    "\n",
    "pos_label = 'down_clicks'\n",
    "labels=['down_clicks','up_clicks']\n",
    "\n",
    "print('Accuracy: {:.4f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_test, y_pred, pos_label = pos_label)))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_test, y_pred, pos_label = pos_label)))\n",
    "print('F1 Score: {:.4f}\\n'.format(f1_score(y_test, y_pred, pos_label = pos_label)))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred, labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "        \n",
    "plt.xlabel('\\nPredictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix\\n', fontsize=18)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
