{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d1511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107bfcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd7daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"E:/Jupyter/masterthesis/models/two_classes/mlp_2class_normal.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad78c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"E:/Jupyter/masterthesis/data/json/two_class/normal/all_mfcc_normal.json\"\n",
    "TRAIN_PATH = \"E:/Jupyter/masterthesis/data/json/two_class/normal/train_mfcc_normal.json\"\n",
    "TEST_PATH = \"E:/Jupyter/masterthesis/data/json/two_class/normal/test_mfcc_normal.json\"\n",
    "VAL_PATH = \"E:/Jupyter/masterthesis/data/json/two_class/normal/val_mfcc_normal.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e9d805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(data_path):\n",
    "\n",
    "    with open(data_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    # convert lists to numpy arrays\n",
    "    X = np.array(data[\"mfcc\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "\n",
    "    print(\"Data succesfully loaded!\")\n",
    "\n",
    "    return  X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f7d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(train_path):\n",
    "  \n",
    "    with open(train_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    # convert lists to numpy arrays\n",
    "    X_tr = np.array(data[\"mfcc\"])\n",
    "    y_tr = np.array(data[\"labels\"])\n",
    "\n",
    "    print(\"Train Data succesfully loaded!\")\n",
    "\n",
    "    return  X_tr, y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142bf1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(test_path):\n",
    "\n",
    "    with open(test_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    # convert lists to numpy arrays\n",
    "    X_ts = np.array(data[\"mfcc\"])\n",
    "    y_ts = np.array(data[\"labels\"])\n",
    "\n",
    "    print(\"Test Data succesfully loaded!\")\n",
    "\n",
    "    return  X_ts, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9988226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_val_data(val_path):\n",
    "\n",
    "    with open(val_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    # convert lists to numpy arrays\n",
    "    X_v = np.array(data[\"mfcc\"])\n",
    "    y_v = np.array(data[\"labels\"])\n",
    "\n",
    "    print(\"Validation Data succesfully loaded!\")\n",
    "\n",
    "    return  X_v, y_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439bd0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    \n",
    "    weight_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "    bias_initializer=tf.keras.initializers.Zeros()\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #input layer\n",
    "    model.add(Flatten(input_shape=(X.shape[1], X.shape[2])))\n",
    "    \n",
    "    # 1st dense layer\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer = weight_initializer, bias_initializer = bias_initializer, kernel_regularizer = l2(0.001)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # 2nd dense layer\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer = weight_initializer, bias_initializer = bias_initializer, kernel_regularizer = l2(0.001)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # 3rd dense layer\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer = weight_initializer, bias_initializer = bias_initializer, kernel_regularizer = l2(0.001)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_sample(model, X, y):\n",
    "\n",
    "    # add a dimension to input data for sample - model.predict() expects a 4d array in this case\n",
    "    X = X[np.newaxis, ...] # array shape (1, 130, 13, 1)\n",
    "\n",
    "    # perform prediction\n",
    "    prediction = model.predict(X)\n",
    "\n",
    "    # get index with max value\n",
    "    predicted_index = np.argmax(prediction, axis=1)\n",
    "\n",
    "    print(\"\\nTarget: {}, Predicted label: {}\".format(y, predicted_index))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2942a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    \n",
    "    acc = history.history['categorical_accuracy']\n",
    "    val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bd19fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # load data\n",
    "    X, y = load_all_data(DATA_PATH)\n",
    "    \n",
    "    #load the data from train, test and validation dataset\n",
    "    X_train, y_train = load_train_data(TRAIN_PATH)\n",
    "    X_test, y_test = load_test_data(TEST_PATH)\n",
    "    X_val, y_val = load_val_data(VAL_PATH)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # define input shape to the model\n",
    "    input_shape = (X.shape[1], X.shape[2])\n",
    "    \n",
    "    # build model\n",
    "    model = build_model(input_shape)\n",
    "\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    "                  sample_weight_mode=[None])\n",
    "\n",
    "\n",
    "    # model summary\n",
    "    model.summary()\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    # The training stops if there is no improvement in the validation accuracy after 10 epoch\n",
    "    es = EarlyStopping(monitor='val_categorical_accuracy', patience=25)\n",
    "    \n",
    "    # if there is a better validation accuracy than previous better accuracy, then we save it in the model\n",
    "    chkp = ModelCheckpoint(filepath=MODEL_PATH, verbose=1, save_best_only=True)\n",
    "    \n",
    "    # cnn_normal_log.csv has the accuracy and loss history\n",
    "    log = CSVLogger('E:/Jupyter/masterthesis/logs/two_classes/normal/mlp_2class_normal_log.csv', append=True, separator=' ')\n",
    "    \n",
    "\n",
    "    # train model\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        batch_size=32, \n",
    "                        epochs=100,\n",
    "                        callbacks = [chkp,es,log])\n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "    # initializing time stamp\n",
    "    startTime = time.time()\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "    # Loading the saved Model\n",
    "    model = tf.keras.models.load_model(MODEL_PATH)\n",
    "    model.summary()\n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "    # Evaluation on validation dataset\n",
    "    score_val = model.evaluate(X_val, y_val, verbose=2)\n",
    "    print('\\nValidation loss and Accuracy')\n",
    "    print('------------------------------------')\n",
    "    print(model.metrics_names)\n",
    "    print(model.metrics_names[0], score_val[0])\n",
    "    print(model.metrics_names[1], score_val[1])\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "    # Evaluation on test dataset\n",
    "    score_test = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print('\\nTest loss and Accuracy')\n",
    "    print('-------------------------------------')\n",
    "    print(model.metrics_names)\n",
    "    print(model.metrics_names[0], score_test[0])\n",
    "    print(model.metrics_names[1], score_test[1])\n",
    "    print('\\n')\n",
    "    \n",
    "    # pick a sample to predict from the test set\n",
    "    X_to_predict = X_test[1550]\n",
    "    y_to_predict = y_test[1550]\n",
    "\n",
    "    # prediction for a single sample sample\n",
    "    predict_single_sample(model, X_to_predict, y_to_predict)\n",
    "    \n",
    "    # take the entire test set for prediction\n",
    "    pred = model.predict(X_test, batch_size = 32, verbose = 2)\n",
    "    p_i = np.argmax(pred, axis=1)\n",
    "    print(\"\\n Target: {}, Predicted label: {}\".format(y_test, p_i))\n",
    "    \n",
    "    results=pd.DataFrame({\"Target\":y_test,\n",
    "                      \"Predicted Labels\":p_i})\n",
    "    results.to_csv(\"E:/Jupyter/masterthesis/predictions/two_class/normal/mlp_2class_normal_result.csv\",index=False)\n",
    "\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "    executionTime = (time.time() - startTime)\n",
    "    print('\\nExecution time in seconds: ' + str(executionTime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7a5f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot accuracy and error as a function of the epochs\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df62e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "pred_file = \"E:/Jupyter/masterthesis/predictions/two_class/normal/mlp_2class_normal_result.csv\"\n",
    "df = pd.read_csv(pred_file)\n",
    "\n",
    "y_test = df[\"Target\"]\n",
    "y_pred = df[\"Predicted Labels\"]\n",
    "\n",
    "pos_label = 0\n",
    "\n",
    "labels=[ 0,1]\n",
    "\n",
    "print('Accuracy: {:.4f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Precision: {:.4f}'.format(precision_score(y_test, y_pred, pos_label = pos_label)))\n",
    "print('Recall: {:.4f}'.format(recall_score(y_test, y_pred, pos_label = pos_label)))\n",
    "print('F1 Score: {:.4f}\\n'.format(f1_score(y_test, y_pred, pos_label = pos_label)))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred, labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "\n",
    "plt.xlabel('Predictions', fontsize=15)\n",
    "plt.ylabel('Actuals', fontsize=15)\n",
    "plt.title('Confusion Matrix\\n', fontsize=18)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
